"""
Internal Scrape Shopping Agent Workflow

This module implements a new LangGraph workflow using internal scrape.py tools:
1. User query analysis
2. Question classification (general vs. search-required)
3. Search execution with search_detailed_products tool
4. Data compatibility parsing for legacy format
5. Final response generation

This workflow provides better performance and control over data extraction.
"""

# Load environment variables at module level to ensure they're available
import time
from dotenv import load_dotenv
from pydantic import BaseModel, Field

from backend.prompts.system_prompts import INTERNAL_SEARCH_QUERY_OPTIMIZATION_PROMPT
from backend.utils.model import load_chat_model
load_dotenv()

import operator
from typing import Dict, Any, List, Literal, TypedDict, Annotated
from langchain_core.messages import BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver

from ..nodes.query_nodes import QueryNodes
from ..nodes.response_nodes import ResponseNodes
from ..nodes.question_nodes import QuestionNodes
from ..tools.scrape import search_detailed_products


class InternalWorkflowState(TypedDict):
    """Enhanced state model for the internal scrape workflow"""
    
    # User input and query analysis
    messages: Annotated[List[BaseMessage], operator.add]
    query_type: Literal["general", "search_required"]
    
    # Search phase - optimized for internal tools
    search_query: str
    search_parameters: Dict[str, Any]  # Changed from str to Dict for internal tools
    search_results: List[Dict[str, Any]]  # Direct product data instead of URLs
    search_metadata: Dict[str, Any]
    
    # Response generation
    final_response: str
    suggested_questions: List[str]
    
    # Workflow control
    current_step: str

class SearchParameters(BaseModel):
    gender: str = Field(
        default="A",
        description="ëŒ€ìƒ ì„±ë³„ í•„í„°. M(ë‚¨ì„±), F(ì—¬ì„±), A(ì „ì²´). ì›í”¼ìŠ¤ ë“± íŠ¹ì • ì„±ë³„ ì•„ì´í…œì€ ìžë™ ì¶”ë¡  ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        examples=["M", "F", "A"]
    )
    minPrice: int = Field(
        default=0,
        ge=0,
        description="ê²€ìƒ‰í•  ìµœì†Œ ê°€ê²© (ì› ë‹¨ìœ„). '5ë§Œì› ì´ìƒ', '10ë§Œì›ëŒ€' ë“±ì˜ í‘œí˜„ì„ ìˆ«ìžë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ 0ì€ ê°€ê²© í•˜í•œ ì—†ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
        examples=[0, 50000, 100000, 200000]
    )
    maxPrice: int = Field(
        default=999999,
        description="ê²€ìƒ‰í•  ìµœëŒ€ ê°€ê²© (ì› ë‹¨ìœ„). '10ë§Œì› ì´í•˜', '20ë§Œì›ëŒ€' ë“±ì˜ í‘œí˜„ì„ ìˆ«ìžë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ 999999ëŠ” ê°€ê²© ìƒí•œ ì—†ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
        examples=[50000, 100000, 200000, 500000, 999999]
    )
    color: str = Field(
        default="",
        description="ìƒ‰ìƒ í•„í„°. í•œêµ­ì–´('ê²€ì€ìƒ‰', 'í•˜ì–€ìƒ‰') ë˜ëŠ” ì˜ì–´('black', 'white') ìƒ‰ìƒì„ í‘œí˜„í•©ë‹ˆë‹¤. ë¹ˆ ë¬¸ìžì—´ì´ë©´ ëª¨ë“  ìƒ‰ìƒì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
        examples=["BLACK", "WHITE", "NAVY", "GRAY", "RED"]
    )
    
    shoeSize: int = Field(
        default=0,
        description="ì‹ ë°œ ì‚¬ì´ì¦ˆ (mm ë‹¨ìœ„). ì‹ ë°œ ê´€ë ¨ ê²€ìƒ‰ì–´ê°€ ìžˆì„ ë•Œë§Œ ì‚¬ìš©í•˜ë©°, ì‹ ë°œì´ ì•„ë‹Œ ê²½ìš° ë°˜ë“œì‹œ 0ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.",
        examples=[0, 230, 250, 270, 280]
    )


class InternalQueryOptimizationOutput(BaseModel):
    
    search_query: str = Field(
        description="í•µì‹¬ ê²€ìƒ‰ì–´ (ìƒí’ˆëª…, ì¹´í…Œê³ ë¦¬ ë“± ê¸°ë³¸ ê²€ìƒ‰ í‚¤ì›Œë“œ)"
    )
    search_parameters: SearchParameters = Field(
        description="ê²€ìƒ‰ í•„í„° íŒŒë¼ë¯¸í„°"
    )


class InternalScrapeNodes:
    """Nodes specifically for internal scrape workflow"""
    
    def __init__(self, model_name: str = "openai/gpt-4.1"):
        self.llm = load_chat_model(model_name)

    
    def optimize_search_query_internal(self, state) -> dict:
        """
        Optimize search query for internal scrape tools
        Converts search parameters to format expected by search_detailed_products
        """
        print("ðŸ” ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìµœì í™”í•˜ê³  ìžˆìŠµë‹ˆë‹¤...")

        query_prompt = ChatPromptTemplate.from_messages([
            ("system", INTERNAL_SEARCH_QUERY_OPTIMIZATION_PROMPT),
            ("human", "{query}")
        ])

        try:
            structured_llm = self.llm.with_structured_output(InternalQueryOptimizationOutput)
            result = structured_llm.invoke(query_prompt.format(query=state["messages"][-1].content))
            
            print(f"Search query optimization result: {result.search_query}, parameters: {result.search_parameters}")
            
            return {
                "search_query": result.search_query,
                "search_parameters": result.search_parameters.model_dump(),
                "current_step": "search_optimized"
            }
        except Exception as e:
            print(f"Error in optimize_search_query: {e}")
            # Extract basic search terms as fallback
            user_query = state["messages"][-1].content
            return {
                "search_query": user_query,
                "search_parameters": {
                    "gender": "A",
                    "minPrice": 0,
                    "maxPrice": 999999,
                    "color": "",
                    "shoeSize": 0
                },
                "current_step": "search_optimized"
            }
    
    
    def search_products_internal(self, state) -> dict:
        """
        Search products using internal search_detailed_products tool
        """
        search_query = state["search_query"]
        search_params = state["search_parameters"]

        try:
            print(f"ðŸ›ï¸ ë¬´ì‹ ì‚¬ì—ì„œ ìƒí’ˆì„ ê²€ìƒ‰í•˜ê³  ìžˆìŠµë‹ˆë‹¤...")
            print(f"ê²€ìƒ‰ì–´: {search_query}")

            search_dict = search_params.copy()
            
            search_dict["keyword"] = search_query  # Add search_query as keyword parameter
            result = search_detailed_products.invoke(search_dict)
            
            if result.get("success"):
                products = result.get("products", [])
                print(f"âœ… {len(products)}ê°œì˜ ìƒí’ˆì„ ì°¾ì•˜ìŠµë‹ˆë‹¤")
                
                # Parse products to legacy-compatible format
                parsed_products = self._parse_to_legacy_format(products)
                print(f"ðŸ“¦ ìƒí’ˆ ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤: {len(parsed_products)}ê°œ")
                
                search_metadata = {
                    "search_query": search_query,
                    "search_parameters": search_dict,  # Use already-converted dict
                    "results_count": len(parsed_products),
                    "search_method": "internal_scrape"
                }
                
                return {
                    "search_results": parsed_products,
                    "search_metadata": search_metadata,
                    "current_step": "search_completed"
                }
            else:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
                return {
                    "search_results": [],
                    "search_metadata": {
                        "search_query": search_query,
                        "search_parameters": search_dict if 'search_dict' in locals() else (search_params if isinstance(search_params, dict) else {}),
                        "results_count": 0,
                        "error": result.get("error")
                    },
                    "current_step": "search_completed"
                }
                
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # Handle search_params serialization in exception case too
            if hasattr(search_params, 'model_dump'):
                search_params_dict = search_params.model_dump()
            elif isinstance(search_params, dict):
                search_params_dict = search_params
            else:
                search_params_dict = {}
                
            return {
                "search_results": [],
                "search_metadata": {
                    "search_query": search_query,
                    "search_parameters": search_params_dict,
                    "results_count": 0,
                    "error": str(e)
                },
                "current_step": "search_completed"
            }
    
    def _parse_to_legacy_format(self, products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Parse internal scrape results to legacy format for compatibility
        
        Args:
            products: Results from search_detailed_products
            
        Returns:
            List of products in legacy format
        """
        legacy_products = []
        
        for product in products:
            try:
                # Map internal format to legacy format
                legacy_product = {
                    # Basic product info
                    "goods_no": str(product.get("goodsNo", "")),
                    "name": product.get("goodsName", ""),
                    "brand": product.get("brandName", ""),
                    "price": product.get("price", 0),
                    "original_price": product.get("normalPrice", 0),
                    "sale_rate": product.get("saleRate", 0),
                    "image_url": product.get("thumbnail", ""),
                    "product_url": product.get("goodsLinkUrl", ""),
                    "is_sold_out": product.get("isSoldOut", False),
                    
                    # Review info
                    "review_count": product.get("reviewCount", 0),
                    "review_score": product.get("reviewScore", 0),
                    
                    # Additional legacy fields
                    "gender": product.get("displayGenderText", ""),
                    "brand_code": product.get("brand", ""),
                    "brand_url": product.get("brandLinkUrl", ""),
                    "is_plus_delivery": product.get("isPlusDelivery", False),
                    
                    # Enhanced data from internal tool
                    "benefits": product.get("benefits", []),
                    "reviews": product.get("reviews", []),
                    "benefits_loaded": product.get("benefits_success", False),
                    "reviews_loaded": product.get("reviews_success", False),
                    
                    # Data source identifier
                    "data_source": "internal_scrape"
                }
                
                legacy_products.append(legacy_product)
                
            except Exception as e:
                print(f"Error parsing product to legacy format: {e}")
                continue
        
        return legacy_products


class InternalScrapeWorkflow:
    """Main workflow class for internal scrape-based shopping assistance"""
    
    def __init__(self, model_name: str = "openai/gpt-4.1"):
        self.query_nodes = QueryNodes(model_name)
        self.internal_nodes = InternalScrapeNodes(model_name)
        self.response_nodes = ResponseNodes(model_name)
        self.question_nodes = QuestionNodes(model_name)
        
        self.saver = InMemorySaver()
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """Build the internal scrape workflow graph"""
        workflow = StateGraph(InternalWorkflowState)
        
        # Add workflow nodes
        workflow.add_node("analyze_query", self.query_nodes.analyze_query)
        workflow.add_node("handle_general_query", self.query_nodes.handle_general_query)
        workflow.add_node("optimize_search_query", self.internal_nodes.optimize_search_query_internal)
        workflow.add_node("search_products", self.internal_nodes.search_products_internal)
        workflow.add_node("generate_final_response", self.response_nodes.generate_final_response)
        workflow.add_node("generate_suggested_questions", self.question_nodes.generate_suggested_questions)
        
        # Set entry point
        workflow.set_entry_point("analyze_query")
        
        # Add conditional edges
        workflow.add_conditional_edges(
            "analyze_query",
            self._route_after_analysis,
            {
                "general": "handle_general_query",
                "search_required": "optimize_search_query"
            }
        )
        
        workflow.add_edge("handle_general_query", "generate_suggested_questions")
        workflow.add_edge("optimize_search_query", "search_products")
        
        workflow.add_conditional_edges(
            "search_products",
            self._route_after_search,
            {
                "generate_response": "generate_final_response",
                "no_results": "generate_final_response"
            }
        )
        
        workflow.add_edge("generate_final_response", "generate_suggested_questions")
        workflow.add_edge("generate_suggested_questions", END)

        return workflow.compile(checkpointer=self.saver)
    
    def _route_after_analysis(self, state: InternalWorkflowState) -> str:
        """Route based on query analysis"""
        return state["query_type"]
    
    def _route_after_search(self, state: InternalWorkflowState) -> str:
        """Route based on search results"""
        if not state["search_results"]:
            return "no_results"
        return "generate_response"


# Create global internal workflow instance
internal_scrape_workflow = InternalScrapeWorkflow("openai/gpt-4.1")

# Export compiled workflow for use
internal_workflow = internal_scrape_workflow.workflow