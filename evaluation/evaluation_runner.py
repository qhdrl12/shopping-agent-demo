"""
Simple Evaluation-Only Execution System

ìºì‹œëœ ì¶”ë¡  ê²°ê³¼ë¡œ í‰ê°€ë§Œ ì‹¤í–‰í•˜ëŠ” ê°„ë‹¨í•œ ì‹œìŠ¤í…œ
"""

import asyncio
import time
from typing import Dict, Any, Optional
from datetime import datetime
from langsmith import Client
from langsmith.evaluation import aevaluate

from evaluation.cache_manager import cache_manager
from evaluation.evaluators import get_parallel_evaluators


class EvaluationRunner:
    """í‰ê°€ ì‹¤í–‰ê¸°"""
    
    def __init__(self, langsmith_client: Optional[Client] = None):
        self.client = langsmith_client or Client()
        self.cache = cache_manager
        self.evaluators = get_parallel_evaluators()  # ê¸°ë³¸ì ìœ¼ë¡œ ë³‘ë ¬ í‰ê°€ì ì‚¬ìš©
    
    async def run_evaluation_only(self,
                                dataset_name: str,
                                experiment_name: Optional[str] = None,
                                max_concurrency: int = 3) -> str:
        """
        ìºì‹œëœ ê²°ê³¼ë¡œ í‰ê°€ë§Œ ì‹¤í–‰ (ê°„ë‹¨í•œ ë²„ì „)
        """
        
        print(f"âš¡ Starting evaluation-only mode for dataset: {dataset_name}")
        print("=" * 60)
        
        # ì‹¤í—˜ ì´ë¦„ ìƒì„±
        if not experiment_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"eval_only_{dataset_name}_{timestamp}"
        
        print(f"ğŸš€ Starting cached evaluation...")
        start_time = time.time()
        
        try:
            # ê°„ë‹¨í•œ ë°©ë²•: ìºì‹œëœ ë°ì´í„°ë¥¼ ì§ì ‘ í‰ê°€
            # LangSmith ë°ì´í„°ì…‹ ë¡œë“œ
            dataset = self.client.read_dataset(dataset_name=dataset_name)
            examples = list(self.client.list_examples(dataset_id=dataset.id))
            
            print(f"ğŸ“Š Found {len(examples)} examples in dataset")
            
            # ê° ì˜ˆì œì— ëŒ€í•´ ìºì‹œëœ ê²°ê³¼ë¡œ í‰ê°€ ì‹¤í–‰
            evaluation_results = []
            
            for example in examples:
                example_id = str(example.id)
                inputs = example.inputs if hasattr(example, 'inputs') else {}
                
                # ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë“œ
                cached_result = self.cache.load_inference_result(dataset_name, example_id)
                
                if cached_result is None:
                    print(f"âš ï¸ No cached result for example {example_id}, skipping...")
                    continue
                
                # ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
                print(f"ğŸš€ Running parallel evaluation for example {example_id}...")
                parallel_results = await self.evaluators.evaluate_parallel(inputs, cached_result)
                
                for eval_result in parallel_results:
                    evaluation_results.append({
                        "example_id": example_id,
                        "evaluator": eval_result["key"],
                        "result": eval_result
                    })
            
            print(f"\nğŸ‰ Direct evaluation completed!")
            print(f"ğŸ“Š Processed: {len(evaluation_results)} evaluations")
            
            evaluation_time = time.time() - start_time
            print(f"â±ï¸  Evaluation time: {evaluation_time:.1f}s")
            
            return f"direct_eval_{experiment_name}"
            
        except Exception as e:
            print(f"âŒ Evaluation failed: {e}")
            raise
    
    def _create_cached_target_function(self, dataset_name: str):
        """ê°„ë‹¨í•œ ìºì‹œ ê¸°ë°˜ target function"""
        
        def cached_target_function(inputs: Dict[str, Any]) -> Dict[str, Any]:
            # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì˜ˆì œ ID ì°¾ê¸°
            example_id = None
            
            print(f"cached_target_function input : {inputs}")
            # ë°©ë²• 1: inputsì—ì„œ ì§ì ‘
            if "example_id" in inputs:
                example_id = inputs["example_id"]
            
            # ë°©ë²• 2: LangSmith ë‚´ë¶€ êµ¬ì¡° (ì‹¤í–‰í•´ë³´ë©´ì„œ í™•ì¸ í•„ìš”)
            # ì‹¤ì œë¡œëŠ” LangSmithê°€ ì–´ë–»ê²Œ example_idë¥¼ ì „ë‹¬í•˜ëŠ”ì§€ í™•ì¸ í›„ ìˆ˜ì •
            
            if not example_id:
                # ì„ì‹œ: ì¿¼ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì°¾ê¸° (ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ì‘ë™)
                query = (inputs.get("query") or 
                        inputs.get("input") or 
                        list(inputs.values())[0] if inputs else "")
                
                # ìºì‹œì—ì„œ ì¿¼ë¦¬ ë§¤ì¹­ìœ¼ë¡œ ì°¾ê¸° (ì„ì‹œ ë°©ë²•)
                cached_examples = self.cache.get_cached_example_ids(dataset_name)
                print(f"cached_examples: {cached_examples}")
                for ex_id in cached_examples:
                    cached_result = self.cache.load_inference_result(dataset_name, ex_id)
                    if cached_result and cached_result.get("query") == query:
                        example_id = ex_id
                        break
            
            if not example_id:
                raise ValueError(f"Cannot find cached result for inputs: {list(inputs.keys())}")
            
            # ìºì‹œì—ì„œ ê²°ê³¼ ë¡œë“œ
            cached_result = self.cache.load_inference_result(dataset_name, example_id)
            
            if cached_result is None:
                raise ValueError(f"No cached result found for example: {example_id}")
            
            # LangSmith í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
            return {
                "output": cached_result.get("final_response", ""),
                **cached_result  # ëª¨ë“  í‰ê°€ ë°ì´í„° í¬í•¨
            }
        
        return cached_target_function


# í¸ì˜ í•¨ìˆ˜
async def run_evaluation_only(dataset_name: str,
                            experiment_name: Optional[str] = None,
                            max_concurrency: int = 3) -> str:
    """í‰ê°€ ì „ìš© ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    runner = EvaluationRunner()
    return await runner.run_evaluation_only(
        dataset_name=dataset_name,
        experiment_name=experiment_name,
        max_concurrency=max_concurrency
    )