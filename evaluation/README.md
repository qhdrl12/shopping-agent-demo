# Shopping Agent Evaluation System

LangSmith ê¸°ë°˜ ë¬´ì‹ ì‚¬ ì‡¼í•‘ ì—ì´ì „íŠ¸ í‰ê°€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê°œì„  ê³¼ì •ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

- **ìë™í™”ëœ í‰ê°€**: LangSmith ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë©° ì—ì´ì „íŠ¸ ì¶”ë¡  ë° í‰ê°€ ìˆ˜í–‰
- **ë‹¤ì¤‘ ë©”íŠ¸ë¦­**: ì›Œí¬í”Œë¡œìš° ì‹¤í–‰, ê²€ìƒ‰ ì •í™•ë„, ë°ì´í„° ì¶”ì¶œ, ì‘ë‹µ í’ˆì§ˆ, ì „ì²´ ì„±ëŠ¥ ë“± 5ê°€ì§€ í‰ê°€ ì§€í‘œ
- **ì‹¤í—˜ ê´€ë¦¬**: LangSmith Experimentsì— ê²°ê³¼ ì €ì¥ ë° ë¹„êµ ë¶„ì„
- **íŠ¸ë Œë“œ ë¶„ì„**: ì‹œê°„ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™” ì¶”ì 
- **ìƒì„¸ ë¦¬í¬íŒ…**: ê°œì„ ì  ë„ì¶œì„ ìœ„í•œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangSmith      â”‚    â”‚  Evaluation     â”‚    â”‚  LangSmith      â”‚
â”‚  Datasets       â”‚â”€â”€â”€â–¶â”‚  Engine         â”‚â”€â”€â”€â–¶â”‚  Experiments    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Agent System   â”‚
                       â”‚  (LangGraph)    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  Evaluation     â”‚
                       â”‚  Metrics        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­

### 1. Workflow Execution (20%)
- ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜ ì •í™•ì„±
- ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” í’ˆì§ˆ
- ë„êµ¬ ì‚¬ìš© íš¨ìœ¨ì„±
- ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬

### 2. Search Accuracy (25%)
- ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„±
- í•„í„°ë§ íš¨ê³¼ì„±
- ìƒí’ˆ ë§í¬ í’ˆì§ˆ

### 3. Data Extraction (20%)
- ì •ë³´ ì™„ì„±ë„
- ë°ì´í„° ì •í™•ì„±
- êµ¬ì¡°í™”ëœ í˜•ì‹

### 4. Response Quality (25%)
- ì¶”ì²œ ì ì ˆì„±
- ì •ë³´ ìœ ìš©ì„±
- ì‘ë‹µ í¬ê´„ì„±
- ì‹¤ìš©ì  ì‹¤í–‰ê°€ëŠ¥ì„±

### 5. Overall Performance (10%)
- ì‹¤í–‰ ì†ë„
- ì˜¤ë¥˜ ì²˜ë¦¬
- ì‚¬ìš©ì ê²½í—˜
- ì‹œìŠ¤í…œ ì‹ ë¢°ì„±

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì¹˜
./scripts/setup_evaluation.sh

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼)
OPENAI_API_KEY=your_openai_api_key_here
LANGSMITH_API_KEY=your_langsmith_api_key_here
LANGSMITH_PROJECT=musinsa-shopping-agent
FIRECRAWL_API_KEY=your_firecrawl_api_key_here
```

### 2. ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸

```bash
# ê°œë³„ ì¿¼ë¦¬ í‰ê°€
python scripts/run_evaluation.py --mode single --query "ë‚˜ì´í‚¤ ìš´ë™í™” ì¶”ì²œí•´ì¤˜"
```

### 3. ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€

```bash
# ì „ì²´ í‰ê°€ ì‹¤í–‰
python scripts/run_evaluation.py --mode full --dataset shopping_agent_dataset

# ë³‘ë ¬ ì²˜ë¦¬ (ì£¼ì˜: ì•ˆì •ì„±ì„ ìœ„í•´ 1-2ë¡œ ì œí•œ ê¶Œì¥)
python scripts/run_evaluation.py --mode full --dataset shopping_agent_dataset --concurrency 1

# ìƒ˜í”Œ í¬ê¸° ì œí•œ
python scripts/run_evaluation.py --mode full --dataset shopping_agent_dataset --sample-size 10
```

### 4. ì‹¤í—˜ ë¹„êµ

```bash
# ë‘ ì‹¤í—˜ ë¹„êµ
python scripts/run_evaluation.py --mode compare --experiments exp1 exp2

# íŠ¸ë Œë“œ ë¶„ì„
python scripts/run_evaluation.py --mode trend --days 30
```

### 5. ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±

```bash
# ë¦¬í¬íŠ¸ ìƒì„±
python scripts/run_evaluation.py --mode report --experiment experiment_name --output report.md
```

## ğŸ“š ì‚¬ìš© ì˜ˆì‹œ

### ê¸°ë³¸ í‰ê°€ ì›Œí¬í”Œë¡œìš°

```python
from evaluation.runner import ShoppingAgentRunner
from evaluation.analyzer import EvaluationAnalyzer

# 1. í‰ê°€ ì‹¤í–‰
runner = ShoppingAgentRunner()
experiment_id = runner.run_evaluation("shopping_agent_dataset")

# 2. ê²°ê³¼ ë¶„ì„
analyzer = EvaluationAnalyzer()
results = analyzer.get_experiment_results(experiment_id)

# 3. ë¦¬í¬íŠ¸ ìƒì„±
report = analyzer.generate_detailed_report(experiment_id)
print(report)
```

### ê°œì„  ì¶”ì 

```python
from evaluation.analyzer import PerformanceTracker

tracker = PerformanceTracker()
improvements = tracker.track_improvements(
    baseline_experiment="baseline_exp",
    current_experiment="improved_exp"
)

print(f"Overall improvement: {improvements['overall_improvement']}")
print(f"Improved metrics: {improvements['improved_metrics']}")
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### í‰ê°€ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
# evaluation/config.py íŒŒì¼ì—ì„œ ì„¤ì • ë³€ê²½
from evaluation.config import EvaluationConfig

config = EvaluationConfig()
config.evaluation_model = "gpt-4o-mini"  # ë” ë¹ ë¥¸ í‰ê°€ìš©
config.default_max_concurrency = 2      # ë³‘ë ¬ ì²˜ë¦¬ ì¦ê°€
```

### ì»¤ìŠ¤í…€ í‰ê°€ì ì¶”ê°€

```python
from evaluation.evaluators import BaseEvaluator

class CustomEvaluator(BaseEvaluator):
    def __init__(self):
        super().__init__()
        self.name = "custom_metric"
        self.max_score = 100
    
    def evaluate(self, inputs, outputs, reference=None):
        # ì»¤ìŠ¤í…€ í‰ê°€ ë¡œì§
        return {
            "key": self.name,
            "score": 0.85,
            "comment": "Custom evaluation result"
        }
```

## ğŸ“ ë””ë ‰í„°ë¦¬ êµ¬ì¡°

```
evaluation/
â”œâ”€â”€ __init__.py              # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”œâ”€â”€ README.md               # ì´ ë¬¸ì„œ
â”œâ”€â”€ config.py               # ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ evaluators.py           # í‰ê°€ í•¨ìˆ˜ë“¤
â”œâ”€â”€ runner.py               # í‰ê°€ ì‹¤í–‰ ì—”ì§„
â”œâ”€â”€ analyzer.py             # ê²°ê³¼ ë¶„ì„ ë„êµ¬
â”œâ”€â”€ results/                # í‰ê°€ ê²°ê³¼ ì €ì¥
â”œâ”€â”€ reports/                # ìƒì„±ëœ ë¦¬í¬íŠ¸
â”œâ”€â”€ datasets/               # ë¡œì»¬ ë°ì´í„°ì…‹
â””â”€â”€ templates/              # ë¦¬í¬íŠ¸ í…œí”Œë¦¿

scripts/
â”œâ”€â”€ run_evaluation.py       # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ setup_evaluation.sh     # ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ test_evaluation.py      # í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ logs/                   # ì‹¤í–‰ ë¡œê·¸
```

## ğŸ›ï¸ ëª…ë ¹ì–´ ë ˆí¼ëŸ°ìŠ¤

### run_evaluation.py ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `--mode single` | ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€ | `--query "ìš´ë™í™” ì¶”ì²œ"` |
| `--mode full` | ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€ | `--dataset dataset_name` |
| `--mode compare` | ì‹¤í—˜ ë¹„êµ | `--experiments exp1 exp2` |
| `--mode trend` | íŠ¸ë Œë“œ ë¶„ì„ | `--days 30` |
| `--mode report` | ë¦¬í¬íŠ¸ ìƒì„± | `--experiment exp_name` |
| `--mode list-datasets` | ë°ì´í„°ì…‹ ëª©ë¡ | |
| `--mode create-sample` | ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± | |

### ê³µí†µ ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--concurrency N` | ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜ | 1 |
| `--sample-size N` | í‰ê°€í•  ì˜ˆì‹œ ìˆ˜ ì œí•œ | ì „ì²´ |
| `--verbose` | ìƒì„¸ ì¶œë ¥ | False |
| `--output FILE` | ì¶œë ¥ íŒŒì¼ ì§€ì • | stdout |

## ğŸ” íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. í™˜ê²½ ë³€ìˆ˜ ì˜¤ë¥˜
```bash
âŒ Missing required environment variables: LANGSMITH_API_KEY
```
**í•´ê²°**: `.env` íŒŒì¼ì— í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

#### 2. í‰ê°€ ì‹¤í–‰ ì‹¤íŒ¨
```bash
âŒ Evaluation failed: No dataset found
```
**í•´ê²°**: LangSmithì— ë°ì´í„°ì…‹ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
```bash
python scripts/run_evaluation.py --mode list-datasets
```

#### 3. ë™ì‹œ ì‹¤í–‰ ì˜¤ë¥˜
```bash
âŒ Rate limit exceeded
```
**í•´ê²°**: `--concurrency 1`ë¡œ ìˆœì°¨ ì‹¤í–‰

#### 4. ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**: `--sample-size` ì˜µì…˜ìœ¼ë¡œ í‰ê°€ í¬ê¸° ì œí•œ

### ì„±ëŠ¥ ìµœì í™”

1. **í‰ê°€ ëª¨ë¸ ì„ íƒ**:
   - `gpt-4o`: ìµœê³  í’ˆì§ˆ (ëŠë¦¼, ë¹„ìŒˆ)
   - `gpt-4o-mini`: ê· í˜• (ê¶Œì¥)

2. **ë³‘ë ¬ ì²˜ë¦¬**:
   - ì•ˆì •ì„±: `--concurrency 1`
   - ì„±ëŠ¥: `--concurrency 2-3` (ì£¼ì˜)

3. **ë°°ì¹˜ í¬ê¸°**:
   - í…ŒìŠ¤íŠ¸: `--sample-size 10`
   - ì „ì²´ í‰ê°€: ìƒ˜í”Œ í¬ê¸° ì œí•œ ì—†ìŒ

## ğŸ“ˆ ê°œì„  ë°©ë²•ë¡ 

### 1. ê¸°ì¤€ì„  ì„¤ì •
```bash
# ì´ˆê¸° í‰ê°€ ì‹¤í–‰
python scripts/run_evaluation.py --mode full --dataset baseline_dataset --experiment baseline_v1
```

### 2. ë³€ê²½ ì‚¬í•­ ì ìš© í›„ ì¬í‰ê°€
```bash
# ê°œì„ ëœ ë²„ì „ í‰ê°€
python scripts/run_evaluation.py --mode full --dataset baseline_dataset --experiment improved_v1
```

### 3. ì„±ëŠ¥ ë¹„êµ
```bash
# ê°œì„  íš¨ê³¼ ë¶„ì„
python scripts/run_evaluation.py --mode compare --experiments baseline_v1 improved_v1
```

### 4. ì§€ì†ì  ëª¨ë‹ˆí„°ë§
```bash
# ì£¼ê¸°ì  íŠ¸ë Œë“œ ë¶„ì„
python scripts/run_evaluation.py --mode trend --days 30
```

## ğŸ¤ ê¸°ì—¬ ê°€ì´ë“œ

í‰ê°€ ì‹œìŠ¤í…œ ê°œì„ ì— ê¸°ì—¬í•˜ë ¤ë©´:

1. **ìƒˆë¡œìš´ í‰ê°€ ë©”íŠ¸ë¦­ ì¶”ê°€**: `evaluation/evaluators.py`ì— ìƒˆ í´ë˜ìŠ¤ êµ¬í˜„
2. **ë¶„ì„ ë„êµ¬ í™•ì¥**: `evaluation/analyzer.py`ì— ìƒˆ ë¶„ì„ í•¨ìˆ˜ ì¶”ê°€
3. **ë¦¬í¬íŠ¸ ê°œì„ **: `evaluation/templates/` ë””ë ‰í„°ë¦¬ì— ìƒˆ í…œí”Œë¦¿ ì¶”ê°€

## ğŸ“ ì§€ì›

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ë©´:

1. **ë¡œê·¸ í™•ì¸**: `scripts/logs/evaluation.log`
2. **í…ŒìŠ¤íŠ¸ ì‹¤í–‰**: `python scripts/test_evaluation.py`
3. **ì„¤ì • ê²€ì¦**: `python -c "from evaluation.config import validate_all_configs; print(validate_all_configs())"`

---

**ê°œë°œíŒ€**: Shopping Agent Team  
**ì—…ë°ì´íŠ¸**: 2024ë…„ 12ì›”  
**ë²„ì „**: 1.0.0