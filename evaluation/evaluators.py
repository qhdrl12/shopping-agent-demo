"""
Enhanced Shopping Agent Evaluators

Based on the reference implementation but improved for the Musinsa shopping agent:
- Uses 0-100 point scale for each metric
- LLM-based evaluation with structured prompts
- Domain-specific criteria for shopping agents
- Comprehensive error handling and debugging
"""

import json
import re
import asyncio
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from langchain_core.language_models import BaseLanguageModel
from langchain_openai import ChatOpenAI


class EvaluationResult(BaseModel):
    """Structured evaluation result"""
    total_score: float = Field(description="ì´ ì ìˆ˜ (0-100 ë²”ìœ„)", ge=0, le=100)
    reasoning: str = Field(description="í‰ê°€ ê·¼ê±° ë° ì„¸ë¶€ ì„¤ëª…")


class BaseEvaluator:
    """Base class for all evaluators"""
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        self.model = model or ChatOpenAI(
            model="gpt-4o",
            temperature=0.1
        )
    
    def __call__(self, run, example):
        """Make evaluator callable for LangSmith"""
        print(f"example: {example}")
        inputs = example.inputs if hasattr(example, 'inputs') else run.inputs
        outputs = run.outputs if hasattr(run, 'outputs') else run
        return self.evaluate(inputs, outputs)
        
    async def _safe_evaluate_async(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Safely evaluate with structured output and error handling (async version)"""
        try:
            # Use structured output for reliable score extraction
            structured_llm = self.model.with_structured_output(EvaluationResult)
            result = await structured_llm.ainvoke(prompt)
            
            return {
                "score": result.total_score,
                "reasoning": result.reasoning,
                "success": True,
                "error": None
            }
        except Exception as e:
            return {
                "score": 0.0,
                "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "success": False,
                "error": str(e)
            }
    
    def _safe_evaluate(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Safely evaluate with structured output and error handling (sync version for compatibility)"""
        try:
            # Use structured output for reliable score extraction
            structured_llm = self.model.with_structured_output(EvaluationResult)
            result = structured_llm.invoke(prompt)
            
            return {
                "score": result.total_score,
                "reasoning": result.reasoning,
                "success": True,
                "error": None
            }
        except Exception as e:
            return {
                "score": 0.0,
                "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "success": False,
                "error": str(e)
            }


class WorkflowExecutionEvaluator(BaseEvaluator):
    """
    Evaluates the workflow execution quality
    Based on tool selection and execution efficiency
    """
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        super().__init__(model)
        self.name = "workflow_execution"
        self.max_score = 100
    
    def evaluate(self, inputs: Dict[str, Any], outputs: Dict[str, Any], 
                 reference: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Evaluate workflow execution based on:
        - Query type classification accuracy (25ì )
        - Search query optimization quality (25ì ) 
        - Filtering accuracy (25ì )
        - Error handling and recovery (25ì )
        """
        
        query = inputs.get("query", "")
        query_type = outputs.get("query_type", "")
        search_query = outputs.get("search_query", "")
        current_step = outputs.get("current_step", "")
        
        # Extract enhanced tracking data for better evaluation
        execution_trace = outputs.get("execution_trace", {})
        workflow_stats = outputs.get("workflow_stats", {})
        messages = outputs.get("messages", [])
        tool_calls = execution_trace.get("tool_calls", [])
        nodes_executed = execution_trace.get("nodes_executed", [])
        execution_path = workflow_stats.get("execution_path", [])
        
        # Extract filtering data for accuracy evaluation
        search_results = outputs.get("search_results", [])
        filtered_links = outputs.get("filtered_product_links", [])
        product_data = outputs.get("product_data", [])
        
        search_count = len(search_results) if search_results else 0
        filtered_count = len(filtered_links) if filtered_links else 0
        extracted_count = len(product_data) if product_data else 0
        
        prompt = f"""
ë‹¹ì‹ ì€ ì‡¼í•‘ ì—ì´ì „íŠ¸ì˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"
ë¶„ë¥˜ëœ ì¿¼ë¦¬ íƒ€ì…: {query_type}
ìƒì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬: "{search_query}"
í˜„ì¬ ë‹¨ê³„: {current_step}

=== ìƒì„¸ ì‹¤í–‰ ì •ë³´ ===
ì‹¤í–‰ ê²½ë¡œ: {' â†’ '.join(execution_path)}
ì´ ì‹¤í–‰ ë‹¨ê³„: {workflow_stats.get('total_steps', 0)}ê°œ
ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜: {len(tool_calls)}íšŒ
ìƒì„±ëœ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ

=== í•„í„°ë§ ê²°ê³¼ ===
ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {search_count}ê°œ
í•„í„°ë§ëœ ë§í¬ ìˆ˜: {filtered_count}ê°œ
ì¶”ì¶œëœ ìƒí’ˆ ìˆ˜: {extracted_count}ê°œ
í•„í„°ë§ ë¹„ìœ¨: {(filtered_count/search_count*100) if search_count > 0 else 0:.1f}%

ë„êµ¬ í˜¸ì¶œ ìƒì„¸:
{json.dumps(tool_calls, ensure_ascii=False, indent=2)}

ë…¸ë“œ ì‹¤í–‰ ìˆœì„œ:
{json.dumps([{'node': n['node'], 'step': n['step'], 'current_step': n['current_step']} for n in nodes_executed], ensure_ascii=False, indent=2)}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ì„ í‰ê°€í•´ì£¼ì„¸ìš” (ì´ 100ì ):

1. ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜ ì •í™•ì„± (25ì )
   - general vs search_required ë¶„ë¥˜ê°€ ì ì ˆí•œê°€?
   - ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í–ˆëŠ”ê°€?

2. ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” í’ˆì§ˆ (25ì )
   - ì›ë³¸ ì¿¼ë¦¬ê°€ ë¬´ì‹ ì‚¬ ê²€ìƒ‰ì— ì í•©í•˜ê²Œ ë³€í™˜ë˜ì—ˆëŠ”ê°€?
   - ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ ë³´ì¡´ë˜ì—ˆëŠ”ê°€?
   - ë¶ˆí•„ìš”í•œ ë‹¨ì–´ê°€ ì œê±°ë˜ì—ˆëŠ”ê°€?

3. í•„í„°ë§ ì •í™•ì„± (25ì )
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìƒí’ˆ ë§í¬ë¥¼ ì •í™•íˆ í•„í„°ë§í–ˆëŠ”ê°€?
   - ê´€ë ¨ì„± ì—†ëŠ” ê²°ê³¼ë¥¼ ì ì ˆíˆ ì œê±°í–ˆëŠ”ê°€?

4. ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬ (25ì )
   - ì˜ˆì™¸ ìƒí™©ì„ ì˜ ì²˜ë¦¬í–ˆëŠ”ê°€?
   - ì ì ˆí•œ fallback ì „ëµì´ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ì ìˆ˜]

ì„¸ë¶€ ì ìˆ˜:
- ì¿¼ë¦¬ íƒ€ì… ë¶„ë¥˜: [0-25ì ]
- ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”: [0-25ì ]  
- í•„í„°ë§ ì •í™•ì„±: [0-25ì ]
- ì˜¤ë¥˜ ì²˜ë¦¬: [0-25ì ]

í‰ê°€ ì´ìœ :
[ê° í•­ëª©ë³„ í‰ê°€ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
"""
        
        result = self._safe_evaluate(prompt, {
            "query": query,
            "query_type": query_type,
            "search_query": search_query,
            "current_step": current_step
        })
        
        return {
            "key": self.name,
            "score": result["score"] / 100.0,  # Normalize to 0-1
            "comment": result["reasoning"]
        }


class SearchAccuracyEvaluator(BaseEvaluator):
    """
    Evaluates search and filtering accuracy
    """
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        super().__init__(model)
        self.name = "search_accuracy"
        self.max_score = 100
    
    def evaluate(self, inputs: Dict[str, Any], outputs: Dict[str, Any], 
                 reference: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Evaluate search accuracy based on:
        - Query relevance (60ì )
        - Result availability (40ì )
        """
        
        query = outputs.get("query", "")
        search_results = outputs.get("search_results", [])
        filtered_links = outputs.get("filtered_product_links", [])
        search_metadata = outputs.get("search_metadata", {})
        
        search_count = len(search_results) if search_results else 0
        filtered_count = len(filtered_links) if filtered_links else 0
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ì™€ íŒŒë¼ë¯¸í„° ë¶„ì„
        search_query = search_metadata.get('search_query', '')
        search_params = search_metadata.get('search_parameters', '')
        search_url = search_metadata.get('search_url', 'N/A')
        
        prompt = f"""
ë‹¹ì‹ ì€ ì‡¼í•‘ ì—ì´ì „íŠ¸ì˜ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

=== ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ===
ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"

=== ì‹œìŠ¤í…œ ê²€ìƒ‰ ìˆ˜í–‰ ë‚´ì—­ ===
ê²€ìƒ‰ì–´: "{search_query}"
ê²€ìƒ‰ íŒŒë¼ë¯¸í„°: {search_params if search_params else 'ì—†ìŒ'}
ìµœì¢… ê²€ìƒ‰ URL: {search_url}
ê²€ìƒ‰ ê²°ê³¼: {search_count}ê°œ ìƒí’ˆ í˜ì´ì§€ ë°œê²¬
ìœ íš¨ ë§í¬: {filtered_count}ê°œ ìƒí’ˆ ë§í¬ í™•ë³´

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš” (ì´ 100ì ):

1. ì¿¼ë¦¬ í•´ì„ ë° ë³€í™˜ í’ˆì§ˆ (60ì )
   - ì‚¬ìš©ì ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í–ˆëŠ”ê°€?
   - í•µì‹¬ í‚¤ì›Œë“œê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
   - ê°€ê²©, ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬ ë“± ì¡°ê±´ì´ ì˜¬ë°”ë¥´ê²Œ ì ìš©ë˜ì—ˆëŠ”ê°€?
   - ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì˜ëª»ëœ ê²€ìƒ‰ì–´ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
   
   ğŸš¨ ë…¼ë¦¬ ì˜¤ë¥˜ íŒ¨ë„í‹° (ì‹¬ê°ë„ë³„):
   - minPrice > maxPrice ê°™ì€ ë…¼ë¦¬ì  ëª¨ìˆœ: -40ì  (ì¹˜ëª…ì  ì˜¤ë¥˜)
   - ì‚¬ìš©ì ì˜ë„ì™€ ì •ë°˜ëŒ€ ê²°ê³¼ë¥¼ ì´ˆë˜í•˜ëŠ” ì˜¤ë¥˜: -35ì 
   - ìŒìˆ˜ ê°€ê²©, ë¹„í˜„ì‹¤ì  ê°€ê²©(1ì–µì› ì´ìƒ): -20ì   
   - ë¸Œëœë“œ í•„ë“œì— ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ: -15ì 
   - ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°ë‚˜ ì˜ëª»ëœ í˜•ì‹: -10ì 
   
   âš ï¸ ì¤‘ìš”: ë…¼ë¦¬ ì˜¤ë¥˜ë¡œ ì¸í•´ ì‚¬ìš©ì ìš”ì²­ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ìƒí’ˆì´ 
   ê²€ìƒ‰ë˜ëŠ” ê²½ìš° í•´ë‹¹ ê²€ìƒ‰ì€ "ì‹¤íŒ¨"ë¡œ ê°„ì£¼
   
   í‰ê°€ ê¸°ì¤€ (íŒ¨ë„í‹° ì ìš© í›„):
   - ì™„ë²½í•œ ì˜ë„ íŒŒì•… + ì ì ˆí•œ íŒŒë¼ë¯¸í„°: 55-60ì 
   - ëŒ€ì²´ë¡œ ì ì ˆí•˜ì§€ë§Œ ì¼ë¶€ ë¶€ì¡±: 40-54ì   
   - í•µì‹¬ ë‚´ìš© ëˆ„ë½ ë˜ëŠ” ë¶€ì ì ˆí•œ ë³€í™˜: 20-39ì 
   - ì˜ë„ì™€ ë‹¤ë¥¸ ê²€ìƒ‰ ë˜ëŠ” ì¤‘ìš” ì¡°ê±´ ë¬´ì‹œ: 0-19ì 
   - ë…¼ë¦¬ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°: ìœ„ ì ìˆ˜ì—ì„œ íŒ¨ë„í‹° ì°¨ê°

2. ê²€ìƒ‰ ê²°ê³¼ í™•ë³´ (40ì )
   - ì¶©ë¶„í•œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ìˆì—ˆëŠ”ê°€?
   - 3ê°œ ì´ìƒ: ë§Œì (40ì ), 1-2ê°œ: 20ì , 0ê°œ: 0ì 

í˜„ì¬ ê²€ìƒ‰ ì„±ê³¼: {filtered_count}ê°œ ìƒí’ˆ í™•ë³´ {'(ì¶©ë¶„)' if filtered_count >= 3 else '(ë¶€ì¡±)' if filtered_count > 0 else '(ì‹¤íŒ¨)'}

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ì ìˆ˜]

ì„¸ë¶€ ì ìˆ˜:
- ì¿¼ë¦¬ í•´ì„ ë° ë³€í™˜ í’ˆì§ˆ: [0-60ì ]
- ê²€ìƒ‰ ê²°ê³¼ í™•ë³´: [0-40ì ]

í‰ê°€ ì´ìœ :
[ë‹¤ìŒ ìˆœì„œë¡œ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ë˜, ë™ì¼í•œ ì˜¤ë¥˜ì— ëŒ€í•´ ì¤‘ë³µ ì–¸ê¸‰ ê¸ˆì§€]

1. ì‚¬ìš©ì ì˜ë„ íŒŒì•… ì •í™•ì„±
   - í•µì‹¬ í‚¤ì›Œë“œ ì¸ì‹ ì—¬ë¶€
   - ê°€ê²©, ë¸Œëœë“œ ë“± ì¡°ê±´ ì´í•´ë„

2. ê²€ìƒ‰ì–´ ë° íŒŒë¼ë¯¸í„° ë³€í™˜ í’ˆì§ˆ  
   - ê²€ìƒ‰ì–´ ì ì ˆì„±
   - íŒŒë¼ë¯¸í„° ì„¤ì •ì˜ ì •í™•ì„±
   - **ë…¼ë¦¬ ì˜¤ë¥˜ ê²€ì¦ í¬í•¨** (minPrice vs maxPrice, ì˜ë„ ì¼ì¹˜ì„± ë“±)

3. íŒ¨ë„í‹° ì ìš© (í•´ë‹¹ë˜ëŠ” ê²½ìš°ë§Œ)
   - ë°œê²¬ëœ ì˜¤ë¥˜ ìœ í˜•ê³¼ ì‹¬ê°ë„
   - ì ìš©ëœ íŒ¨ë„í‹° ì ìˆ˜ì™€ ì´ìœ 
   - ì‚¬ìš©ì ê²½í—˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

4. ìµœì¢… ì¢…í•© í‰ê°€
   - ê¸°ë³¸ ì ìˆ˜ ì‚°ì • ê·¼ê±°
   - íŒ¨ë„í‹° ì°¨ê° í›„ ìµœì¢… ì ìˆ˜
   - ê°œì„  ë°©í–¥ ì œì•ˆ

âš ï¸ ì£¼ì˜: ë™ì¼í•œ ì˜¤ë¥˜(ì˜ˆ: ê°€ê²© ë²”ìœ„ ëª¨ìˆœ)ë¥¼ ì—¬ëŸ¬ í•­ëª©ì—ì„œ ë°˜ë³µ ì–¸ê¸‰í•˜ì§€ ë§ê³ , 
í•œ ë²ˆë§Œ ëª…í™•íˆ ê¸°ìˆ í•˜ì—¬ íŒ¨ë„í‹° ì ìš©
"""
        
        result = self._safe_evaluate(prompt, {
            "query": query,
            "search_count": search_count,
            "filtered_count": filtered_count
        })
        
        return {
            "key": self.name,
            "score": result["score"] / 100.0,
            "comment": result["reasoning"]
        }


class DataExtractionEvaluator(BaseEvaluator):
    """
    Evaluates product data extraction quality
    """
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        super().__init__(model)
        self.name = "data_extraction"
        self.max_score = 100
    
    def evaluate(self, inputs: Dict[str, Any], outputs: Dict[str, Any], 
                 reference: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Evaluate data extraction based on:
        - Extraction success rate (50ì )
        - Information completeness (25ì )
        - Data accuracy (15ì )
        - Structured formatting (10ì )
        """
        
        query = inputs.get("query", "")
        product_data = outputs.get("product_data", [])
        
        product_count = len(product_data) if product_data else 0
        
        # Extract success rate: ì‹œìŠ¤í…œ ì •ì±…ì„ ê³ ë ¤í•œ ì¶”ì¶œ ì„±ê³µë¥ 
        filtered_links = outputs.get("filtered_product_links", [])
        total_attempted = len(filtered_links)
        
        # ì‹œìŠ¤í…œ ì •ì±…: ìµœëŒ€ 3ê°œ ìƒí’ˆë§Œ ì‚¬ìš©ìì—ê²Œ ì œê³µ
        max_system_limit = 3
        effective_attempted = min(total_attempted, max_system_limit)
        
        # ì‹¤ì œ ì¶”ì¶œ ì„±ê³µ ê°œìˆ˜ (ì‹œìŠ¤í…œ ì œí•œ ì „)
        raw_extracted = outputs.get("extracted_products_count", product_count)
        # ì‹œìŠ¤í…œ ì œí•œì„ ê³ ë ¤í•œ ì„±ê³µ ê°œìˆ˜
        successfully_extracted = min(raw_extracted, max_system_limit) if raw_extracted else product_count
        
        success_rate = (successfully_extracted / effective_attempted) if effective_attempted > 0 else 0.0
        
        # Check required fields
        required_fields = ["name", "price", "original_price", "discount_info", "reward_points", "images"]
        optional_fields = ["shipping_info", "size_info", "stock_status", "rating", "colors", "description", "features", "review_count"]
        
        completeness_stats = []
        if product_data:
            for i, product in enumerate(product_data, 1):
                required_present = sum(1 for field in required_fields if product.get(field))
                optional_present = sum(1 for field in optional_fields if product.get(field))
                completeness_stats.append({
                    "product_index": i,
                    "required": required_present,
                    "optional": optional_present,
                    "completeness_ratio": f"{required_present}/{len(required_fields)}"
                })
        
        # Format product data for display
        formatted_products = ""
        for i, product in enumerate(product_data, 1):
            formatted_products += f"\n{i}. ìƒí’ˆ{i} ì¶”ì¶œ ë°ì´í„°\n{json.dumps(product, ensure_ascii=False, indent=2)}\n"
        
        prompt = f"""
ë‹¹ì‹ ì€ ì‡¼í•‘ ì—ì´ì „íŠ¸ì˜ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"

=== ì‹œìŠ¤í…œ ì •ì±… ===
- ì‚¬ìš©ìì—ê²ŒëŠ” ìµœëŒ€ 3ê°œ ìƒí’ˆë§Œ ì œê³µ (ì‘ë‹µ ì†ë„ ë° í’ˆì§ˆ ìµœì í™”)
- ì‹¤ì œë¡œëŠ” ë” ë§ì€ ìƒí’ˆì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŒ

=== ì¶”ì¶œ ê²°ê³¼ ===
ì „ì²´ ê²€ìƒ‰ëœ ë§í¬: {total_attempted}ê°œ
ì‹œìŠ¤í…œ ì²˜ë¦¬ ëŒ€ìƒ: {effective_attempted}ê°œ (ìµœëŒ€ 3ê°œ ì œí•œ)
ì‹¤ì œ ì¶”ì¶œ ì„±ê³µ: {raw_extracted if raw_extracted else 'N/A'}ê°œ
ì‚¬ìš©ì ì œê³µ: {successfully_extracted}ê°œ
ì¶”ì¶œ ì„±ê³µë¥ : {success_rate:.1%} (ì²˜ë¦¬ ëŒ€ìƒ ê¸°ì¤€)

ì¶”ì¶œí•œ ìƒí’ˆ ë°ì´í„°:{formatted_products}

ì™„ì„±ë„ í†µê³„:
- í•„ìˆ˜ í•„ë“œ: {required_fields}
- ì„ íƒ í•„ë“œ: {optional_fields}
- ê° ìƒí’ˆë³„ ì™„ì„±ë„: {completeness_stats}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš” (ì´ 100ì ):

1. ì¶”ì¶œ ì„±ê³µë¥  (50ì )
   - ì‹œìŠ¤í…œ ì²˜ë¦¬ ëŒ€ìƒ ìƒí’ˆ ëŒ€ë¹„ ì„±ê³µì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•œ ë¹„ìœ¨
   - ì‹œìŠ¤í…œ ì •ì±…: ì†ë„/í’ˆì§ˆ ìµœì í™”ë¥¼ ìœ„í•´ ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ ì²˜ë¦¬
   - 100%: 50ì , 80%: 40ì , 60%: 30ì , 40%: 20ì , 20%: 10ì , 0%: 0ì 
   - í˜„ì¬ ì„±ê³µë¥ : {success_rate:.1%} ({successfully_extracted}/{effective_attempted})

2. ì •ë³´ ì™„ì„±ë„ (25ì )
   - í•„ìˆ˜ ì •ë³´(ìƒí’ˆëª…, ê°€ê²©, ë¸Œëœë“œ, ì´ë¯¸ì§€, URL)ê°€ ëª¨ë‘ ì¶”ì¶œë˜ì—ˆëŠ”ê°€?
   - ì¶”ê°€ ì •ë³´(ì„¤ëª…, ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ ë“±)ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?

3. ë°ì´í„° ì •í™•ì„± (15ì )
   - ì¶”ì¶œëœ ì •ë³´ê°€ ì •í™•í•˜ê³  ì¼ê´€ì„±ì´ ìˆëŠ”ê°€?
   - ê°€ê²© í˜•ì‹ì´ ì˜¬ë°”ë¥¸ê°€?
   - URLì´ ìœ íš¨í•œê°€?

4. êµ¬ì¡°í™”ëœ í˜•ì‹ (10ì )
   - ë°ì´í„°ê°€ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì–´ ìˆëŠ”ê°€?
   - í•„ë“œëª…ì´ ëª…í™•í•˜ê³  í‘œì¤€í™”ë˜ì–´ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ì ìˆ˜]

ì„¸ë¶€ ì ìˆ˜:
- ì¶”ì¶œ ì„±ê³µë¥ : [0-50ì ]
- ì •ë³´ ì™„ì„±ë„: [0-25ì ]
- ë°ì´í„° ì •í™•ì„±: [0-15ì ]
- êµ¬ì¡°í™”ëœ í˜•ì‹: [0-10ì ]

í‰ê°€ ì´ìœ :
[ê° í•­ëª©ë³„ í‰ê°€ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
"""
        
        result = self._safe_evaluate(prompt, {
            "query": query,
            "product_count": product_count,
            "completeness_stats": completeness_stats
        })
        
        return {
            "key": self.name,
            "score": result["score"] / 100.0,
            "comment": result["reasoning"]
        }


class ResponseQualityEvaluator(BaseEvaluator):
    """
    Evaluates final response quality and helpfulness
    """
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        super().__init__(model)
        self.name = "response_quality"
        self.max_score = 100
    
    def evaluate(self, inputs: Dict[str, Any], outputs: Dict[str, Any], 
                 reference: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Evaluate response quality based on:
        - Recommendation appropriateness (35ì )
        - Information usefulness (30ì )
        - Response comprehensiveness (25ì )
        - Practical actionability (10ì )
        """
        
        query = inputs.get("query", "")
        final_response = outputs.get("final_response", "")
        suggested_questions = outputs.get("suggested_questions", [])
        product_data = outputs.get("product_data", [])
        
        # Enhanced tracking data
        messages = outputs.get("messages", [])
        execution_trace = outputs.get("execution_trace", {})
        workflow_stats = outputs.get("workflow_stats", {})
        
        response_length = len(final_response)
        suggestion_count = len(suggested_questions) if suggested_questions else 0
        product_count = len(product_data) if product_data else 0
        message_count = len(messages)
        
        prompt = f"""
ë‹¹ì‹ ì€ ì‡¼í•‘ ì—ì´ì „íŠ¸ì˜ ìµœì¢… ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"
ìµœì¢… ì‘ë‹µ ê¸¸ì´: {response_length}ì
ì œì•ˆëœ ì§ˆë¬¸ ìˆ˜: {suggestion_count}ê°œ
í¬í•¨ëœ ìƒí’ˆ ìˆ˜: {product_count}ê°œ
ì´ ë©”ì‹œì§€ ìˆ˜: {message_count}ê°œ

ìµœì¢… ì‘ë‹µ:
{final_response}

ì œì•ˆëœ ì§ˆë¬¸ë“¤:
{json.dumps(suggested_questions, ensure_ascii=False, indent=2)}

=== ëŒ€í™” íë¦„ ë¶„ì„ ===
ë©”ì‹œì§€ ì¶”ì  (ìµœê·¼ 5ê°œ):
{json.dumps(messages[-5:] if messages else [], ensure_ascii=False, indent=2)}

ìƒí’ˆ ì •ë³´ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):
{json.dumps(product_data[:3] if product_data else [], ensure_ascii=False, indent=2)}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš” (ì´ 100ì ):

1. ì¶”ì²œ ì ì ˆì„± (35ì )
   - ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ìƒí’ˆì„ ì¶”ì²œí–ˆëŠ”ê°€?
   - ì¶”ì²œ ì´ìœ ê°€ ëª…í™•í•˜ê³  ì„¤ë“ë ¥ì´ ìˆëŠ”ê°€?
   - ë‹¤ì–‘í•œ ì˜µì…˜ì„ ì œê³µí–ˆëŠ”ê°€?

2. ì •ë³´ ìœ ìš©ì„± (30ì )
   - êµ¬ë§¤ ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì œê³µí–ˆëŠ”ê°€?
   - ê°€ê²©, ë¸Œëœë“œ, íŠ¹ì§• ë“± í•µì‹¬ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
   - ì •ë³´ê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?

3. ì‘ë‹µ í¬ê´„ì„± (25ì )
   - ì‚¬ìš©ì ì§ˆë¬¸ì— ì™„ì „íˆ ë‹µë³€í–ˆëŠ”ê°€?
   - ì¶”ê°€ì ìœ¼ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ì •ë³´ë¥¼ ì œê³µí–ˆëŠ”ê°€?
   - ì „ì²´ì ì¸ êµ¬ì„±ì´ ë…¼ë¦¬ì ì¸ê°€?

4. ì‹¤ìš©ì  ì‹¤í–‰ê°€ëŠ¥ì„± (10ì )
   - ì‹¤ì œ êµ¬ë§¤ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí–ˆëŠ”ê°€?
   - ë‹¤ìŒ ë‹¨ê³„ì— ëŒ€í•œ ì•ˆë‚´ê°€ ìˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ì ìˆ˜]

ì„¸ë¶€ ì ìˆ˜:
- ì¶”ì²œ ì ì ˆì„±: [0-35ì ]
- ì •ë³´ ìœ ìš©ì„±: [0-30ì ]
- ì‘ë‹µ í¬ê´„ì„±: [0-25ì ]
- ì‹¤ìš©ì  ì‹¤í–‰ê°€ëŠ¥ì„±: [0-10ì ]

í‰ê°€ ì´ìœ :
[ê° í•­ëª©ë³„ í‰ê°€ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
"""
        
        result = self._safe_evaluate(prompt, {
            "query": query,
            "response_length": response_length,
            "suggestion_count": suggestion_count,
            "product_count": product_count
        })
        
        return {
            "key": self.name,
            "score": result["score"] / 100.0,
            "comment": result["reasoning"]
        }


class OverallPerformanceEvaluator(BaseEvaluator):
    """
    Evaluates overall performance including latency and user experience
    """
    
    def __init__(self, model: Optional[BaseLanguageModel] = None):
        super().__init__(model)
        self.name = "overall_performance"  
        self.max_score = 100
    
    def evaluate(self, inputs: Dict[str, Any], outputs: Dict[str, Any], 
                 reference: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Evaluate overall performance based on:
        - Execution speed (30ì )
        - Error handling (25ì )
        - User experience (25ì )
        - System reliability (20ì )
        """
        
        query = inputs.get("query", "")
        execution_time = outputs.get("execution_time", 0)
        query_type = outputs.get("query_type", "")
        
        # Performance thresholds based on query type
        if query_type == "general":
            excellent_threshold = 5.0
            good_threshold = 10.0
            acceptable_threshold = 20.0
        else:
            excellent_threshold = 15.0
            good_threshold = 30.0
            acceptable_threshold = 60.0
        
        prompt = f"""
ë‹¹ì‹ ì€ ì‡¼í•‘ ì—ì´ì „íŠ¸ì˜ ì „ì²´ì ì¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì¿¼ë¦¬: "{query}"
ì¿¼ë¦¬ íƒ€ì…: {query_type}
ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ

ì„±ëŠ¥ ê¸°ì¤€:
- ìš°ìˆ˜: {excellent_threshold}ì´ˆ ì´í•˜
- ì–‘í˜¸: {good_threshold}ì´ˆ ì´í•˜  
- í—ˆìš©: {acceptable_threshold}ì´ˆ ì´í•˜

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì „ì²´ ì„±ëŠ¥ì„ í‰ê°€í•´ì£¼ì„¸ìš” (ì´ 100ì ):

1. ì‹¤í–‰ ì†ë„ (30ì )
   - ì‘ë‹µ ì‹œê°„ì´ ì‚¬ìš©ì ê¸°ëŒ€ì¹˜ì— ë¶€í•©í•˜ëŠ”ê°€?
   - ì¿¼ë¦¬ íƒ€ì…ì— ì ì ˆí•œ ì²˜ë¦¬ ì‹œê°„ì¸ê°€?

2. ì˜¤ë¥˜ ì²˜ë¦¬ (25ì )
   - ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í–ˆëŠ”ê°€?
   - ì˜ˆì™¸ ìƒí™©ì„ ì ì ˆíˆ ì²˜ë¦¬í–ˆëŠ”ê°€?

3. ì‚¬ìš©ì ê²½í—˜ (25ì )
   - ì „ì²´ì ì¸ ì‚¬ìš©ì ê²½í—˜ì´ ë§Œì¡±ìŠ¤ëŸ¬ìš´ê°€?
   - ì‘ë‹µì´ ì‚¬ìš©ì ì¹œí™”ì ì¸ê°€?

4. ì‹œìŠ¤í…œ ì‹ ë¢°ì„± (20ì )
   - ì¼ê´€ëœ í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ”ê°€?
   - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë™ì‘ì„ ë³´ì´ëŠ”ê°€?

ì‘ë‹µ í˜•ì‹:
ì ìˆ˜: [0-100 ì‚¬ì´ì˜ ì ìˆ˜]

ì„¸ë¶€ ì ìˆ˜:
- ì‹¤í–‰ ì†ë„: [0-30ì ]
- ì˜¤ë¥˜ ì²˜ë¦¬: [0-25ì ]
- ì‚¬ìš©ì ê²½í—˜: [0-25ì ]
- ì‹œìŠ¤í…œ ì‹ ë¢°ì„±: [0-20ì ]

í‰ê°€ ì´ìœ :
[ê° í•­ëª©ë³„ í‰ê°€ ê·¼ê±°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…]
"""
        
        result = self._safe_evaluate(prompt, {
            "query": query,
            "execution_time": execution_time,
            "query_type": query_type
        })
        
        return {
            "key": self.name,
            "score": result["score"] / 100.0,
            "comment": result["reasoning"]
        }


class ParallelEvaluatorManager:
    """Manager for parallel evaluation execution"""
    
    def __init__(self):
        self.evaluators = [
            SearchAccuracyEvaluator(), 
            DataExtractionEvaluator(),
            ResponseQualityEvaluator(),
            # OverallPerformanceEvaluator()
        ]
    
    async def evaluate_parallel(self, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Run all evaluators in parallel for maximum performance
        """
        print(f"ğŸš€ Starting parallel evaluation with {len(self.evaluators)} evaluators...")
        
        # Create async tasks for all evaluators
        tasks = []
        for evaluator in self.evaluators:
            task = self._run_evaluator_async(evaluator, inputs, outputs)
            tasks.append(task)
        
        # Execute all evaluations in parallel
        start_time = asyncio.get_event_loop().time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = asyncio.get_event_loop().time()
        
        print(f"âš¡ Parallel evaluation completed in {end_time - start_time:.2f}s")
        
        # Process results and handle any exceptions
        evaluation_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ Evaluator {self.evaluators[i].name} failed: {result}")
                evaluation_results.append({
                    "key": self.evaluators[i].name,
                    "score": 0.0,
                    "comment": f"í‰ê°€ ì‹¤íŒ¨: {str(result)}"
                })
            else:
                evaluation_results.append(result)
                print(f"âœ… {result['key']}: {result['score']:.3f}")
        
        return evaluation_results
    
    async def _run_evaluator_async(self, evaluator, inputs: Dict[str, Any], outputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Run a single evaluator asynchronously
        """
        try:
            # Run evaluator in a thread pool to avoid blocking
            import concurrent.futures
            loop = asyncio.get_event_loop()
            
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(evaluator.evaluate, inputs, outputs)
                result = await loop.run_in_executor(None, lambda: future.result())
            
            return result
        except Exception as e:
            raise Exception(f"Evaluation failed for {evaluator.name}: {str(e)}")


# Create parallel evaluator manager instance
parallel_manager = ParallelEvaluatorManager()

# ëª¨ë“  í‰ê°€ìë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ì˜ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)
EVALUATORS = [
    SearchAccuracyEvaluator(), 
    DataExtractionEvaluator(),
    ResponseQualityEvaluator(),
    # OverallPerformanceEvaluator()
]


def get_evaluators():
    """Get all evaluators for the shopping agent"""
    return EVALUATORS


def get_parallel_evaluators():
    """Get parallel evaluator manager"""
    return parallel_manager