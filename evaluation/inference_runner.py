"""
Inference-Only Execution System

ì¶”ë¡ ë§Œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥í•˜ëŠ” ì‹œìŠ¤í…œ:
- ë°ì´í„°ì…‹ë³„ ì¶”ë¡  ì‹¤í–‰ ë° ìºì‹œ ì €ì¥
- ì§„í–‰ë¥  ì¶”ì  ë° ì¤‘ë‹¨/ì¬ê°œ ì§€ì›
- ì‹¤íŒ¨í•œ ì˜ˆì œ ì¬ì‹œë„ ë¡œì§
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
"""

import asyncio
import time
from typing import Dict, Any, List, Optional
from datetime import datetime
from langsmith import Client

from evaluation.cache_manager import cache_manager
from evaluation.runner import ShoppingAgentRunner


class InferenceRunner:
    """ì¶”ë¡  ì „ìš© ì‹¤í–‰ê¸°"""
    
    def __init__(self, langsmith_client: Optional[Client] = None):
        self.client = langsmith_client or Client()
        self.agent_runner = ShoppingAgentRunner(langsmith_client=self.client)
        self.cache = cache_manager
    
    async def run_inference_only(self, 
                                dataset_name: str,
                                max_concurrency: int = 1,
                                sample_size: Optional[int] = None,
                                resume: bool = True) -> Dict[str, Any]:
        """
        ì¶”ë¡ ë§Œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        
        Args:
            dataset_name: LangSmith ë°ì´í„°ì…‹ ì´ë¦„
            max_concurrency: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            sample_size: ì²˜ë¦¬í•  ì˜ˆì œ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)
            resume: ê¸°ì¡´ ìºì‹œëœ ê²°ê³¼ ì¬ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼ í†µê³„
        """
        
        print(f"ğŸ” Starting inference-only mode for dataset: {dataset_name}")
        print(f"âš™ï¸ Max concurrency: {max_concurrency}")
        if sample_size:
            print(f"ğŸ”¢ Sample size: {sample_size}")
        print(f"ğŸ”„ Resume mode: {resume}")
        print("=" * 60)
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        try:
            dataset = self.client.read_dataset(dataset_name=dataset_name)
            print(f"âœ… Dataset loaded: {dataset.name} ({dataset.example_count} examples)")
        except Exception as e:
            print(f"âŒ Error loading dataset: {e}")
            raise
        
        # ìºì‹œ ìƒíƒœ í™•ì¸
        cache_status = self.cache.get_dataset_cache_status(dataset_name)
        print(f"\nğŸ“Š Cache Status:")
        print(f"  Cached examples: {cache_status['completed_examples']}/{cache_status['total_examples']}")
        print(f"  Completion rate: {cache_status['completion_rate']:.1%}")
        print(f"  Version match: {cache_status['version_match']}")
        
        if not cache_status['version_match'] and cache_status['cached']:
            print(f"âš ï¸ Agent version mismatch detected. Cache will be rebuilt.")
            resume = False
        
        # ì²˜ë¦¬í•  ì˜ˆì œë“¤ ìˆ˜ì§‘
        examples = list(self.client.list_examples(
            dataset_id=dataset.id, 
            limit=sample_size
        ))
        
        if resume:
            # ì´ë¯¸ ìºì‹œëœ ì˜ˆì œë“¤ ì œì™¸
            cached_ids = set(self.cache.get_cached_example_ids(dataset_name))
            examples_to_process = [ex for ex in examples if str(ex.id) not in cached_ids]
            print(f"ğŸ”„ Resume mode: {len(examples_to_process)} examples to process")
        else:
            examples_to_process = examples
            print(f"ğŸ†• Fresh start: {len(examples_to_process)} examples to process")
        
        if not examples_to_process:
            print("âœ… All examples already cached and up to date!")
            return {
                "dataset_name": dataset_name,
                "total_examples": len(examples),
                "processed_examples": 0,
                "cached_examples": len(examples),
                "failed_examples": 0,
                "execution_time": 0.0,
                "status": "already_complete"
            }
        
        # ì¶”ë¡  ì‹¤í–‰
        start_time = time.time()
        results = await self._process_examples_batch(
            dataset_name, 
            examples_to_process, 
            max_concurrency
        )
        execution_time = time.time() - start_time
        
        # ê²°ê³¼ ìš”ì•½
        successful = sum(1 for r in results if r["success"])
        failed = len(results) - successful
        
        print(f"\nğŸ‰ Inference completed!")
        print(f"ğŸ“Š Results Summary:")
        print(f"  Total processed: {len(results)}")
        print(f"  Successful: {successful}")
        print(f"  Failed: {failed}")
        print(f"  Success rate: {successful/len(results):.1%}")
        print(f"  Execution time: {execution_time:.1f}s")
        print(f"  Average time per example: {execution_time/len(results):.1f}s")
        
        # ìºì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        final_cache_status = self.cache.get_dataset_cache_status(dataset_name)
        print(f"\nğŸ“ˆ Final Cache Status:")
        print(f"  Total cached: {final_cache_status['completed_examples']}")
        print(f"  Completion rate: {final_cache_status['completion_rate']:.1%}")
        
        return {
            "dataset_name": dataset_name,
            "total_examples": len(examples),
            "processed_examples": len(results),
            "successful_examples": successful,
            "failed_examples": failed,
            "success_rate": successful / len(results) if results else 0.0,
            "execution_time": execution_time,
            "avg_time_per_example": execution_time / len(results) if results else 0.0,
            "cache_completion_rate": final_cache_status["completion_rate"],
            "status": "completed"
        }
    
    async def _process_examples_batch(self, 
                                    dataset_name: str,
                                    examples: List[Any],
                                    max_concurrency: int) -> List[Dict[str, Any]]:
        """ì˜ˆì œë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrency)
        
        # ëª¨ë“  ì˜ˆì œì— ëŒ€í•œ íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for i, example in enumerate(examples):
            task = self._process_single_example_with_semaphore(
                semaphore, dataset_name, example, i + 1, len(examples)
            )
            tasks.append(task)
        
        # ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"âŒ Example {i+1} failed with exception: {result}")
                processed_results.append({
                    "example_id": examples[i].id,
                    "success": False,
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_example_with_semaphore(self,
                                                   semaphore: asyncio.Semaphore,
                                                   dataset_name: str,
                                                   example: Any,
                                                   current: int,
                                                   total: int) -> Dict[str, Any]:
        """ì„¸ë§ˆí¬ì–´ë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì˜ˆì œ ì²˜ë¦¬"""
        async with semaphore:
            return await self._process_single_example(dataset_name, example, current, total)
    
    async def _process_single_example(self,
                                    dataset_name: str,
                                    example: Any,
                                    current: int,
                                    total: int) -> Dict[str, Any]:
        """ë‹¨ì¼ ì˜ˆì œ ì¶”ë¡  ì‹¤í–‰"""
        
        example_id = str(example.id)  # UUIDë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        inputs = example.inputs if hasattr(example, 'inputs') else {}
        
        print(f"ğŸ”„ Processing example {current}/{total}: {example_id}")
        
        try:
            # ì¿¼ë¦¬ ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œëª… ì‹œë„)
            query = (inputs.get("query") or 
                    inputs.get("input") or 
                    inputs.get("question") or 
                    inputs.get("text") or
                    inputs.get("prompt") or
                    "")
            
            # ë‹¨ì¼ ê°’ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
            if not query and len(inputs) == 1:
                query = list(inputs.values())[0]
            
            if not query:
                raise ValueError(f"No query found in inputs: {list(inputs.keys())}")
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰
            start_time = time.time()
            result = await self.agent_runner.run_agent(query)
            inference_time = time.time() - start_time
            
            if result.get("success", False):
                # ìºì‹œì— ì €ì¥
                cache_success = self.cache.save_inference_result(
                    dataset_name, example_id, result
                )
                
                if cache_success:
                    print(f"âœ… Example {current}/{total} completed and cached ({inference_time:.1f}s)")
                    return {
                        "example_id": example_id,
                        "success": True,
                        "inference_time": inference_time,
                        "cached": True
                    }
                else:
                    print(f"âš ï¸ Example {current}/{total} completed but cache failed")
                    return {
                        "example_id": example_id,
                        "success": True,
                        "inference_time": inference_time,
                        "cached": False,
                        "cache_error": "Failed to save to cache"
                    }
            else:
                error_msg = result.get("error", "Unknown error")
                print(f"âŒ Example {current}/{total} failed: {error_msg}")
                return {
                    "example_id": example_id,
                    "success": False,
                    "error": error_msg,
                    "inference_time": inference_time
                }
                
        except Exception as e:
            print(f"âŒ Example {current}/{total} exception: {e}")
            return {
                "example_id": example_id,
                "success": False,
                "error": str(e),
                "inference_time": 0.0
            }
    
    def get_inference_progress(self, dataset_name: str) -> Dict[str, Any]:
        """ì¶”ë¡  ì§„í–‰ë¥  ì¡°íšŒ"""
        cache_status = self.cache.get_dataset_cache_status(dataset_name)
        
        try:
            dataset = self.client.read_dataset(dataset_name=dataset_name)
            total_examples = dataset.example_count
        except Exception:
            total_examples = cache_status.get("total_examples", 0)
        
        return {
            "dataset_name": dataset_name,
            "total_examples": total_examples,
            "completed_examples": cache_status["completed_examples"],
            "progress_percentage": cache_status["completion_rate"] * 100,
            "agent_version": cache_status["agent_version"],
            "version_match": cache_status["version_match"],
            "last_updated": cache_status.get("last_updated")
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
async def run_inference_only(dataset_name: str,
                           max_concurrency: int = 1,
                           sample_size: Optional[int] = None,
                           resume: bool = True) -> Dict[str, Any]:
    """ì¶”ë¡  ì „ìš© ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜"""
    runner = InferenceRunner()
    return await runner.run_inference_only(
        dataset_name=dataset_name,
        max_concurrency=max_concurrency,
        sample_size=sample_size,
        resume=resume
    )


def get_inference_progress(dataset_name: str) -> Dict[str, Any]:
    """ì¶”ë¡  ì§„í–‰ë¥  ì¡°íšŒ í¸ì˜ í•¨ìˆ˜"""
    runner = InferenceRunner()
    return runner.get_inference_progress(dataset_name)