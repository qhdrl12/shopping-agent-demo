#!/bin/bash

# Shopping Agent Evaluation Setup Script
# This script sets up the evaluation environment and dependencies

set -e  # Exit on any error

echo "ðŸš€ Setting up Shopping Agent Evaluation System"
echo "=============================================="

# Check if we're in the right directory
if [ ! -f "pyproject.toml" ]; then
    echo "âŒ Error: Must run from project root directory"
    exit 1
fi

# Check for required environment variables
echo "ðŸ” Checking environment variables..."

required_vars=("OPENAI_API_KEY" "LANGSMITH_API_KEY" "LANGSMITH_PROJECT" "FIRECRAWL_API_KEY")
missing_vars=()

for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        missing_vars+=("$var")
    fi
done

if [ ${#missing_vars[@]} -ne 0 ]; then
    echo "âŒ Missing required environment variables:"
    printf '   - %s\n' "${missing_vars[@]}"
    echo ""
    echo "Please set these variables in your .env file or environment"
    echo "Example .env file:"
    echo ""
    echo "OPENAI_API_KEY=your_openai_api_key_here"
    echo "LANGSMITH_API_KEY=your_langsmith_api_key_here" 
    echo "LANGSMITH_PROJECT=musinsa-shopping-agent"
    echo "FIRECRAWL_API_KEY=your_firecrawl_api_key_here"
    echo ""
    exit 1
fi

echo "âœ… Environment variables OK"

# Install additional dependencies for evaluation
echo "ðŸ“¦ Installing evaluation dependencies..."

# Check if uv is available
if command -v uv &> /dev/null; then
    echo "Using uv for package management..."
    
    # Add evaluation-specific dependencies
    uv add langsmith
    uv add pandas
    uv add matplotlib
    uv add seaborn  # For better visualizations
    
    echo "âœ… Dependencies installed with uv"
else
    echo "Using pip for package management..."
    
    pip install langsmith pandas matplotlib seaborn
    
    echo "âœ… Dependencies installed with pip"
fi

# Create necessary directories
echo "ðŸ“ Creating evaluation directories..."

mkdir -p evaluation/results
mkdir -p evaluation/reports  
mkdir -p evaluation/datasets
mkdir -p evaluation/templates
mkdir -p scripts/logs

echo "âœ… Directories created"

# Set up logging configuration
echo "ðŸ“ Setting up logging..."

cat > evaluation/logging.conf << EOF
[loggers]
keys=root,evaluation

[handlers]
keys=consoleHandler,fileHandler

[formatters]
keys=simpleFormatter,detailedFormatter

[logger_root]
level=INFO
handlers=consoleHandler

[logger_evaluation]
level=DEBUG
handlers=consoleHandler,fileHandler
qualname=evaluation
propagate=0

[handler_consoleHandler]
class=StreamHandler
level=INFO
formatter=simpleFormatter
args=(sys.stdout,)

[handler_fileHandler]
class=FileHandler
level=DEBUG
formatter=detailedFormatter
args=('scripts/logs/evaluation.log', 'a')

[formatter_simpleFormatter]
format=%(levelname)s - %(message)s

[formatter_detailedFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
EOF

echo "âœ… Logging configured"

# Create a sample evaluation configuration
echo "âš™ï¸  Creating sample configuration..."

cat > evaluation/eval_config.json << EOF
{
  "evaluation": {
    "model": "gpt-4o",
    "temperature": 0.1,
    "max_concurrency": 1,
    "timeout_seconds": 300
  },
  "metrics": {
    "workflow_execution": {
      "weight": 0.20,
      "description": "How well the agent executes the workflow steps"
    },
    "search_accuracy": {
      "weight": 0.25, 
      "description": "Accuracy of search and filtering processes"
    },
    "data_extraction": {
      "weight": 0.20,
      "description": "Quality of product data extraction"
    },
    "response_quality": {
      "weight": 0.25,
      "description": "Overall quality and helpfulness of responses"
    },
    "overall_performance": {
      "weight": 0.10,
      "description": "System performance including latency"
    }
  },
  "thresholds": {
    "excellent_latency_general": 5.0,
    "good_latency_general": 10.0, 
    "acceptable_latency_general": 20.0,
    "excellent_latency_search": 15.0,
    "good_latency_search": 30.0,
    "acceptable_latency_search": 60.0,
    "low_performance_threshold": 0.5
  }
}
EOF

echo "âœ… Configuration created"

# Create report template
echo "ðŸ“„ Creating report template..."

cat > evaluation/templates/report_template.md << 'EOF'
# Shopping Agent Evaluation Report

**Experiment:** {{experiment_name}}  
**Generated:** {{timestamp}}  
**Total Runs:** {{run_count}}

## Executive Summary

{{executive_summary}}

## Metrics Overview

| Metric | Score | Status |
|--------|-------|--------|
{{#metrics}}
| {{name}} | {{score}} | {{status}} |
{{/metrics}}

## Detailed Results

{{detailed_results}}

## Performance Issues

{{performance_issues}}

## Recommendations

{{recommendations}}

---
*Report generated by Shopping Agent Evaluation System*
EOF

echo "âœ… Report template created"

# Create a quick test script
echo "ðŸ§ª Creating test script..."

cat > scripts/test_evaluation.py << 'EOF'
#!/usr/bin/env python3
"""
Quick test script to verify evaluation setup
"""

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_imports():
    """Test that all evaluation modules can be imported"""
    try:
        from evaluation.evaluators import get_evaluators
        from evaluation.runner import ShoppingAgentRunner
        from evaluation.analyzer import EvaluationAnalyzer
        from evaluation.config import get_evaluation_config
        
        print("âœ… All evaluation modules imported successfully")
        return True
    except Exception as e:
        print(f"âŒ Import error: {e}")
        return False

def test_environment():
    """Test environment setup"""
    required_vars = ["OPENAI_API_KEY", "LANGSMITH_API_KEY", "LANGSMITH_PROJECT", "FIRECRAWL_API_KEY"]
    missing = [var for var in required_vars if not os.getenv(var)]
    
    if missing:
        print(f"âŒ Missing environment variables: {', '.join(missing)}")
        return False
    
    print("âœ… Environment variables OK")
    return True

def test_configuration():
    """Test configuration loading"""
    try:
        config = get_evaluation_config()
        issues = config.validate()
        
        if issues:
            print(f"âš ï¸  Configuration issues: {', '.join(issues)}")
            return False
        
        print("âœ… Configuration valid")
        return True
    except Exception as e:
        print(f"âŒ Configuration error: {e}")
        return False

def main():
    """Run all tests"""
    print("ðŸ§ª Testing Evaluation Setup")
    print("=" * 40)
    
    tests = [
        ("Environment", test_environment),
        ("Imports", test_imports), 
        ("Configuration", test_configuration)
    ]
    
    passed = 0
    for name, test_func in tests:
        print(f"\nðŸ” Testing {name}...")
        if test_func():
            passed += 1
    
    print(f"\nðŸ“Š Results: {passed}/{len(tests)} tests passed")
    
    if passed == len(tests):
        print("ðŸŽ‰ All tests passed! Evaluation system is ready.")
        return 0
    else:
        print("âŒ Some tests failed. Please check the issues above.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
EOF

chmod +x scripts/test_evaluation.py

echo "âœ… Test script created"

# Run the test script
echo "ðŸ§ª Running setup validation..."
python scripts/test_evaluation.py

if [ $? -eq 0 ]; then
    echo ""
    echo "ðŸŽ‰ Evaluation system setup complete!"
    echo ""
    echo "Next steps:"
    echo "1. Create or configure your LangSmith dataset"
    echo "2. Run a test evaluation: python scripts/run_evaluation.py --mode single --query 'ë‚˜ì´í‚¤ ìš´ë™í™” ì¶”ì²œí•´ì¤˜'"
    echo "3. Run full evaluation: python scripts/run_evaluation.py --mode full --dataset your_dataset_name"
    echo ""
    echo "ðŸ“š Available commands:"
    echo "  python scripts/run_evaluation.py --help"
    echo ""
else
    echo ""
    echo "âŒ Setup validation failed. Please check the errors above."
    exit 1
fi